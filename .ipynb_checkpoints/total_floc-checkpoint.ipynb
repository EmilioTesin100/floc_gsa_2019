{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to find the total floc value over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=20)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = []\n",
    "filename = []\n",
    "scene_tag = []\n",
    "deployment = []\n",
    "start_frame = []\n",
    "end_frame = []\n",
    "\n",
    "with open('/home/jovyan/floc_gsa_2019/region_listed.txt') as f:\n",
    "    for line in f:\n",
    "        with open(line.strip()) as l:\n",
    "            data = json.load(l)\n",
    "            for region in data['regions']:\n",
    "                if region['type'] == 'static':\n",
    "                    try:\n",
    "                        if '_p2_z0' in region['sceneTag']:\n",
    "                            url.append('https://rawdata.oceanobservatories.org/files' + data['movie']['URL'])\n",
    "                            filename.append((data['movie']['URL'].split('/')[-1]))\n",
    "                            scene_tag.append(region['sceneTag'])\n",
    "                            deployment.append(int(scene_tag[-1].split('_')[0][1]))                            \n",
    "                            start_frame.append(int(region['startFrame']))\n",
    "                            end_frame.append(int(region['endFrame']))\n",
    "                    except:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_windows = pd.DataFrame({'filename': filename, 'url': url, 'scene_tag': scene_tag, 'deployment': deployment, 'start_frame': start_frame, 'end_frame': end_frame})\n",
    "scene_windows.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_interval = 150\n",
    "window_buffer = 20\n",
    "frame_lists = []\n",
    "for i, start_frame in enumerate(scene_windows.start_frame):\n",
    "    frame_lists.append(list(range(start_frame+window_buffer, scene_windows.end_frame[i]-window_buffer, frame_interval)))\n",
    "scene_windows['frame_list'] = frame_lists\n",
    "scene_windows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frame_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrappers to deal with potential get_moov_atom and get_frame timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycamhd as camhd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moov_atom_timeout(filename):\n",
    "    try:\n",
    "        return camhd.get_moov_atom(filename)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_timeout(filename, frame_number, pix_fmt, moov_atom):\n",
    "    if moov_atom:\n",
    "        try:\n",
    "            return camhd.get_frame(filename, frame_number, pix_fmt, moov_atom)\n",
    "        except:\n",
    "            return np.zeros((1080, 1920), dtype=np.uint16)\n",
    "    else:\n",
    "        return np.zeros((1080, 1920), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a delayed Dask array of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.array as dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_frame_list = []\n",
    "for i, row in scene_windows[scene_windows.deployment == 5].iterrows():\n",
    "    filename = row.url\n",
    "    delayed_moov_atom = delayed(get_moov_atom_timeout)(filename)\n",
    "    for frame_number in row.frame_list:\n",
    "        delayed_frame = delayed(get_frame_timeout)(filename, frame_number, 'gray16le', delayed_moov_atom)\n",
    "        delayed_frame_list.append(dsa.from_delayed(delayed_frame, (1080, 1920), np.uint16))\n",
    "delayed_frame_array = dsa.stack(delayed_frame_list)\n",
    "delayed_frame_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(delayed_frame_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = delayed_frame_array[0].compute()\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dask array is in many ways like a numpy array, except in this case it holds a set of instructions for how to acquire each chunk of the array, which makes it easy to farm this array out to workers in the cloud using the [distributed](http://distributed.readthedocs.io/en/latest/#) scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the filter that will be used to filter images in the frequency domain\n",
    "To deal with variations in lighting and high-frequency noise, we filter each subimage using a Butterworth bandpass filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 20 # low cut wavenumber\n",
    "d2 = 400 # high cut wavenumber\n",
    "n = 4\n",
    "bff = butterworth(d1, d2, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butterworth(d1, d2, n):\n",
    "    x = np.arange(-1024/2+0.5,1024/2+1-0.5)\n",
    "    xx, yy = np.meshgrid(x, x)\n",
    "    d = np.sqrt(xx**2+yy**2)\n",
    "    bff = (1 - (1./(1 + (d/d1)**(2*n))))*(1/(1 + (d/d2)**(2*n)))\n",
    "    return bff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the floc proxy function\n",
    "The floc proxy is simply the number of pixels in each filtered subimage that have a value greater than 4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_filter(frame, d1, d2, n):\n",
    "    if frame.ndim == 3 and frame.shape[0] == 1:\n",
    "        I = np.squeeze(frame[0, 0:1024, 0:1024])\n",
    "    else:\n",
    "        I = frame[0:1024, 0:1024]\n",
    "    bff = butterworth(d1, d2, n)\n",
    "    I_fft = np.fft.fft2(I)\n",
    "    I_fft_shift = np.fft.fftshift(I_fft)\n",
    "    I_fft_shift_filt = I_fft_shift*bff # filter with the Butterworth filter\n",
    "    I_fft_filt = np.fft.ifftshift(I_fft_shift_filt)\n",
    "    I_filt = np.fft.ifft2(I_fft_filt)\n",
    "    return I_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4000; # this is an arbitrary threshold that seems to work\n",
    "def frame_thresh(frame, d1, d2, n, threshold):\n",
    "    I_filt = frame_filter(frame, d1, d2, n)\n",
    "    I_thresh = np.array(np.absolute(I_filt)>threshold)\n",
    "    return I_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_floc_proxy(frame, d1, d2, n):\n",
    "    return np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_floc_proxy(frame, d1, d2, n):\n",
    "    I_filt = frame_filter(frame, d1, d2, n)\n",
    "    return np.array([(np.absolute(I_filt)>4000).sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## working Backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_proxy = []\n",
    "for i, row in scene_windows[scene_windows.deployment == 5].iterrows():\n",
    "    filename = row.url\n",
    "    delayed_moov_atom = delayed(get_moov_atom_timeout)(filename)\n",
    "    for frame_number in row.frame_list:\n",
    "        delayed_frame = delayed(get_frame_timeout)(filename, frame_number, 'gray16le', delayed_moov_atom)\n",
    "        delayed_frame_thresh = delayed(frame_thresh)(delayed_frame, d1, d2, n, threshold)\n",
    "        floc_proxy.append(delayed(calc_floc_proxy)(delayed_frame_thresh))\n",
    "floc_proxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "floc_volume = compute(*floc_proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, math\n",
    "import matplotlib.dates as dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = []\n",
    "frame_datenum = []\n",
    "frame_datetime = []\n",
    "frame_numbers = []\n",
    "\n",
    "dbcamhd = pd.read_json('/home/jovyan/floc_gsa_2019/dbcamhd.json', orient=\"records\", lines=True)\n",
    "for i, row in scene_windows[scene_windows.deployment == 5].iterrows():\n",
    "    for frame_number in row.frame_list:\n",
    "        url.append(row.url)\n",
    "        frame_epoch_seconds = dbcamhd['timestamp'][dbcamhd.filename == row.url].iloc[0] + frame_number/29.97\n",
    "        frame_datetime.append(datetime.datetime.fromtimestamp(frame_epoch_seconds))\n",
    "        frame_datenum.append(dates.date2num(frame_datetime[-1]))\n",
    "        frame_numbers.append(frame_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_results = pd.DataFrame({'url': url, 'frame_number': frame_numbers, 'timestamp': frame_datenum,\n",
    "                        'datetime': frame_datetime, 'floc': floc_volume})\n",
    "regions_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_floc_for_dep_5.pickle', 'wb') as f:\n",
    "    pickle.dump(regions_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
